{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['The amount of pollution is increasing day by day.',\n",
    "       'The concert was just great.',\n",
    "       'I love to see Gordon Ramsey cook food.',\n",
    "       'Google DeepMind is introducing a new AI Technology.',\n",
    "       'AI robots are examples of great technology present today.',\n",
    "       'All of us were singing in the concert today.',\n",
    "       'We have launched campaigns to stop pollution and global warming.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Dataset\n",
    "dataset = [line.lower() for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the amount of pollution is increasing day by day.',\n",
       " 'the concert was just great.',\n",
       " 'i love to see gordon ramsey cook food.',\n",
       " 'google deepmind is introducing a new ai technology.',\n",
       " 'ai robots are examples of great technology present today.',\n",
       " 'all of us were singing in the concert today.',\n",
       " 'we have launched campaigns to stop pollution and global warming.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TF-IDF for text\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 35)\t0.231608612212116\n",
      "  (0, 2)\t0.326425447033964\n",
      "  (0, 26)\t0.231608612212116\n",
      "  (0, 27)\t0.270961154226111\n",
      "  (0, 21)\t0.270961154226111\n",
      "  (0, 19)\t0.326425447033964\n",
      "  (0, 9)\t0.652850894067928\n",
      "  (0, 5)\t0.326425447033964\n"
     ]
    }
   ],
   "source": [
    "# Document no.: 0, Index of word in BoW Model: 35, TF-IDF Value: 0.231608612212116\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components: Number of concepts, n_iter: Number of iterations\n",
    "lsa = TruncatedSVD(n_components=4, n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21978169, 0.15860868, 0.10022888, 0.01694285, 0.15936535,\n",
       "       0.10022888, 0.01694285, 0.29718798, 0.00386627, 0.20045775,\n",
       "       0.10540448, 0.15936535, 0.00386627, 0.01694285, 0.10540448,\n",
       "       0.00386627, 0.29781608, 0.01694285, 0.15860868, 0.10022888,\n",
       "       0.10540448, 0.17069334, 0.19941208, 0.01694285, 0.00386627,\n",
       "       0.10540448, 0.29672746, 0.09726259, 0.15936535, 0.00386627,\n",
       "       0.15936535, 0.00386627, 0.15860868, 0.01694285, 0.21978169,\n",
       "       0.32514182, 0.01727336, 0.26394575, 0.15860868, 0.01694285,\n",
       "       0.19941208, 0.01694285, 0.15860868])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corresponding to each concept, which are the most important words\n",
    "imp_terms = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ai',\n",
       " 'all',\n",
       " 'amount',\n",
       " 'and',\n",
       " 'are',\n",
       " 'by',\n",
       " 'campaigns',\n",
       " 'concert',\n",
       " 'cook',\n",
       " 'day',\n",
       " 'deepmind',\n",
       " 'examples',\n",
       " 'food',\n",
       " 'global',\n",
       " 'google',\n",
       " 'gordon',\n",
       " 'great',\n",
       " 'have',\n",
       " 'in',\n",
       " 'increasing',\n",
       " 'introducing',\n",
       " 'is',\n",
       " 'just',\n",
       " 'launched',\n",
       " 'love',\n",
       " 'new',\n",
       " 'of',\n",
       " 'pollution',\n",
       " 'present',\n",
       " 'ramsey',\n",
       " 'robots',\n",
       " 'see',\n",
       " 'singing',\n",
       " 'stop',\n",
       " 'technology',\n",
       " 'the',\n",
       " 'to',\n",
       " 'today',\n",
       " 'us',\n",
       " 'warming',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept:  0\n",
      "('the', 0.32514181559191574)\n",
      "('great', 0.29781607563038975)\n",
      "('concert', 0.29718797776320965)\n",
      "('of', 0.29672746406942835)\n",
      "('today', 0.26394575325530384)\n",
      "('technology', 0.21978169272259968)\n",
      "('ai', 0.21978169272259956)\n",
      "('day', 0.20045775332963428)\n",
      "('just', 0.1994120779518987)\n",
      "('was', 0.1994120779518987)\n",
      "\n",
      "Concept:  1\n",
      "('to', 0.3485938243525486)\n",
      "('pollution', 0.24010551449293846)\n",
      "('cook', 0.21080444346498864)\n",
      "('food', 0.21080444346498864)\n",
      "('gordon', 0.21080444346498864)\n",
      "('love', 0.21080444346498864)\n",
      "('ramsey', 0.21080444346498864)\n",
      "('see', 0.21080444346498864)\n",
      "('and', 0.2091446642680138)\n",
      "('campaigns', 0.2091446642680138)\n",
      "\n",
      "Concept:  2\n",
      "('ai', 0.32926821071879864)\n",
      "('technology', 0.32926821071879847)\n",
      "('deepmind', 0.2697281954559743)\n",
      "('google', 0.2697281954559743)\n",
      "('introducing', 0.2697281954559743)\n",
      "('new', 0.2697281954559743)\n",
      "('is', 0.2055757082124677)\n",
      "('are', 0.12693944933958315)\n",
      "('examples', 0.12693944933958315)\n",
      "('present', 0.12693944933958315)\n",
      "\n",
      "Concept:  3\n",
      "('day', 0.43627235382552015)\n",
      "('pollution', 0.2448711875222656)\n",
      "('by', 0.21813617691276008)\n",
      "('increasing', 0.21813617691276008)\n",
      "('amount', 0.21813617691275994)\n",
      "('is', 0.17556655541341218)\n",
      "('the', 0.08812364307404918)\n",
      "('campaigns', 0.0768588273747975)\n",
      "('global', 0.0768588273747975)\n",
      "('have', 0.0768588273747975)\n"
     ]
    }
   ],
   "source": [
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(imp_terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x: x[1], reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    print('\\nConcept: ',i)\n",
    "    for term in sortedTerms:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(imp_terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x: x[1], reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    concept_words['Concept ' + str(i)] = sortedTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0:\n",
      "1.0227847863206128\n",
      "1.3189700248893126\n",
      "0\n",
      "0.43956338544519924\n",
      "1.2980526784003212\n",
      "1.1830030106798575\n",
      "0\n",
      "\n",
      "Concept 1:\n",
      "0.24010551449293846\n",
      "0\n",
      "1.6134204851424805\n",
      "0\n",
      "0\n",
      "0\n",
      "1.0069886673815147\n",
      "\n",
      "Concept 2:\n",
      "0.2055757082124677\n",
      "0\n",
      "0\n",
      "1.9430249114739622\n",
      "1.0393547694563465\n",
      "0\n",
      "0\n",
      "\n",
      "Concept 3:\n",
      "2.035514624399047\n",
      "0.08812364307404918\n",
      "0\n",
      "0.17556655541341218\n",
      "0\n",
      "0.08812364307404918\n",
      "0.47544766964665813\n"
     ]
    }
   ],
   "source": [
    "for key in concept_words.keys():\n",
    "    sentence_scores = []\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        score = 0\n",
    "        for word in words:\n",
    "            for word_with_score in concept_words[key]:\n",
    "                if word == word_with_score[0]:\n",
    "                    score += word_with_score[1]\n",
    "        sentence_scores.append(score)\n",
    "    print('\\n'+key+':')\n",
    "    for sent_score in sentence_scores:\n",
    "        print(sent_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
